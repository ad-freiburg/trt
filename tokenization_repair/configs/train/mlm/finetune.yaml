experiment: "${MODEL_NAME}_tokenization_repair_char"
experiment_dir: ${EXPERIMENT_DIR}

seed: 22

train:
  num_epochs: ${NUM_EPOCHS:1}
  train_data: ${LMDB_PATH}

  in_memory: false
  num_workers: 0

  batch_max_tokens: ${BATCH_MAX_TOKENS}
  max_seq_length: ${MAX_SEQ_LENGTH:512}

  optimizer:
    type: "adamw"
    learning_rate: ${LR:0.00001}
    weight_decay: 0.01

  lr_scheduler:
    type: "multi_step_with_warmup"
    arguments:
      warmup_steps: ${WARMUP_STEPS:16000}
      steps: [0.75, 0.95]
      factor: 0.1

  loss:
    type: "seq2seq_cross_entropy"
    arguments:
      ignore_index: -1
      weights: [1, 5, 5]

  swap_inputs_and_targets: false
  mixed_precision: true
  eval_interval: ${EVAL_INTERVAL:10000}
  log_interval: ${LOG_INTERVAL:2500}
  keep_last_n_checkpoints: 1

val:
  val_data: 5000

  text_metrics:
    - name: "qualitative_batch_evaluation_tokenization_repair"

model:
  name: "${MODEL_NAME}"
  type: "encoder_with_head"

  encoder:
    tokenizer: "char"

    pretrained: ${ENCODER_PRETRAINED}

    embedding_dim: ${MODEL_DIM:512}
    max_num_embeddings: ${MAX_SEQ_LENGTH:512}
    learned_positional_embeddings: false
    norm_embeddings: true
    model_dim: ${MODEL_DIM:512}
    feedforward_dim: ${FFN_DIM:2048}
    attention_heads: ${NUM_HEADS:8}
    dropout: 0.1

    num_layers: ${NUM_LAYERS}
    share_parameters: false
    activation: "gelu"

  head:
    type: "sequence_classification"
    arguments:
      model_dim: ${MODEL_DIM:512}
      num_layers: ${HEAD_NUM_LAYERS:3}
      num_classes: 3
