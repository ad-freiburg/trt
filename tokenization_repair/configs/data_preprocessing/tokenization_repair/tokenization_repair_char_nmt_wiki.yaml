data:
  - "${DATA_INPUT_DIR}/wikidump/*/*.jsonl"

output_dir: "${DATA_OUTPUT_DIR}/char_tokenization_repair/tokenization_repair_nmt_wikidump"

tokenizer: "char"
target_tokenizer: "tokenization_repair"

seed: 22

preprocessing:
  - type: "switch"
    arguments:
      functions:
        - type: "edit_tokens_corruption"
          arguments:
            p: 0.15
            seed: 22
        - type: "skip"
      prob: [0.8, 0.2]
      seed: 22
  - type: "drop"
    arguments:
      keys: ["target_sequence"]
  - type: "switch"
    arguments:
      functions:
        - type: "whitespace_corruption"
          arguments:
            iw_p: 0.1
            dw_p: 0.2
            seed: 22
        - type: "whitespace_corruption"
          arguments:
            no_ws: true
      prob: [ 0.8, 0.2 ]
      seed: 23
  - type: "tokenization_repair"

lmdb: true
lmdb_name: "lmdb"

max_sequences: ${MAX_SEQUENCES}
max_sequence_length: 1024
cut_overflowing: false