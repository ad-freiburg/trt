data:
  - "${DATA_INPUT_DIR}/tokenization_repair/mixed_with_errors/*.jsonl"

output_dir: "${DATA_OUTPUT_DIR}/char_eo/tokenization_repair_arxiv_with_errors"

tokenizer: "char"

seed: 22

preprocessing:
  - type: "substring"
    arguments:
      length: 510
  - type: "switch"
    arguments:
      functions:
        - type: "whitespace_corruption"
          arguments:
            iw_p: 0.1
            dw_p: 0.2
            seed: 22
        - type: "whitespace_corruption"
          arguments:
            no_ws: true
        - type: "whitespace_corruption"
          arguments:
            full_ws: true
      prob: [ 0.8, 0.1, 0.1 ]
      seed: 23
  - type: "tokenization_repair"
    arguments:
      output_type: "label"
      max_length: 510

lmdb: true
lmdb_name: "lmdb"

pretokenize: true
ensure_equal_length: true
max_sequence_length: 512
cut_overflowing: true
