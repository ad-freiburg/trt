## Config for training

# HINT: You can use ${ENV_VAR:default_value} if you want to use environment variables in your config files

# name of the experiment
# (will together with a timestamp form the name of the subfolder
# created in the experiment_dir below)
experiment: "my_experiment"

# directory where experiment is saved
experiment_dir: "experiments"

# global seed (set to null if no seed is wanted)
seed: 22

## Training configuration
train:
  # number of epochs
  num_epochs: 20

  # train_data is path to a single lmdb database
  train_data: "preprocessed/example/lmdb"

  # whether to load data in memory (only use this for small datasets)
  in_memory: false

  # number of processes to use for data loading (if 0 data is loaded on main process)
  num_workers: 4

  # number of samples in one batch
  batch_size: 6

  # maximum number of tokens per batch
  # (uses a custom bucket sampler to put sequences of similar length into one batch,
  # cannot be specified together with batch size)
  batch_max_tokens: 10000

  # minimum number of tokens per sequence
  min_seq_length: 0

  # maximum number of tokens per sequence
  max_seq_length: 512

  # optimizer params
  optimizer:
    # unique optimizer name
    type: "adamw"
    # learning rate
    learning_rate: 0.00001
    # weight decay
    weight_decay: 0.00025

  # learning rate schedule params
  lr_scheduler:
    # unique lr scheduler name
    type: "linear_with_warmup"
    # optional arguments
    arguments:
      warmup_steps: 16000

  # loss function params
  loss:
    # unique loss function name
    type: "cross_entropy"
    # optional arguments
    arguments:
      argument: value

  # whether to transform the data s.t. inputs are targets and targets are inputs
  # (can e.g. be used to train a spelling corruptor)
  swap_inputs_and_targets: false

  # whether to use mixed precision training (FP32 & FP16)
  # can save significant memory and compute time on supported GPUs
  mixed_precision: false

  # evaluate (and checkpoint) model on validation data every n batches
  eval_interval: 5000

  # log training statistics every n batches
  log_interval: 500

  # only keep the last n checkpoints during training (saves disk space), best and last checkpoint are always saved
  keep_last_n_checkpoints: 5

  # resume training from a previous checkpoint (other parts of the config should be the same as the config used to
  # create the checkpoint you want to resume from)
  resume_from_checkpoint: path/to/checkpoint

## Validation configuration
val:
  # val_data has to be one of
  #   - int: number of validation samples randomly sampled from the train data
  #   - float: percentage to randomly sample from train data
  #   - str: path to a single lmdb database
  val_data: "example/preprocessed/val_lmdb"

  # list of text metric configs to apply during validation (see trt/utils/metrics.py)
  text_metrics:
    - name: "qualitative_batch_evaluation"
      # optional arguments
      arguments:
        with_special_tokens: true

## Model configuration
model:
  # name of the model (e.g. used for creating checkpoints)
  name: "my_model"

  # type of the model (one of transformer, encoder_with_head or decoder)
  type: "transformer"

  # optional path to a pretrained checkpoint from which weights are initialized
  pretrained: null

  # optional path to a checkpoint from which training is resumed (differs from pretrained because it also
  # restores the optimizer and lr_scheduler state)
  # if set, overrides pretrained
  resume_from_checkpoint: null

  # share the embedding layer for encoder and decoder
  # (can only be used if embedding related config is the same for encoder and decoder)
  share_encoder_decoder_embeddings: false

  # share the weights from the embedding layer of the decoder
  # with the final linear output projection
  share_decoder_input_output_embeddings: true

  # encoder config
  # (required if model is of type transformer or encoder_with_head)
  encoder:
    # type of encoder
    type: "default"

    # path to a pretrained encoder checkpoint
    pretrained: null

    # path to a pretrained tokenizer to use or name of an existing tokenizer (e.g. char, byte, etc.)
    # (make sure that this is the same tokenizer you
    # used for tokenizing the train_data and val_data)
    tokenizer: "my_pretrained_tokenizer.json"

    # maximum number of embeddings supported (this will be the max sequence length that can be put into the model
    # during inference)
    max_num_embeddings: 1024

    # embedding dimensionality (<= model_dim, if smaller, then factorized embeddings
    # inspired by ALBERT are used)
    embedding_dim: 768

    # whether to learn embeddings or use sinusoidal embeddings
    learned_positional_embeddings: false

    # whether to apply layer normalization on the final embeddings
    norm_embeddings: false

    # dimensionality of the model
    model_dim: 768

    # dimensionality inside the feedforward layers
    feedforward_dim: 2048

    # number of attention heads in each attention mechanism
    attention_heads: 16

    # amount of dropout used in each layer
    dropout: 0.1

    # number of encoder layers
    num_layers: 6

    # whether to share the parameters (weights) between the encoder layers (like ALBERT)
    share_parameters: false

    # activation function to use (one of "relu" or "gelu")
    activation: "gelu"

  # decoder config (see encoder config for the options)
  # (required if model is of type transformer or decoder)
  decoder:
    type: "default"
    pretrained: null
    tokenizer: "my_pretrained_tokenizer.json"
    max_num_embeddings: 1024
    embedding_dim: 768
    learned_positional_embeddings: false
    norm_embeddings: false
    model_dim: 768
    feedforward_dim: 2048
    attention_heads: 16
    dropout: 0.1
    num_layers: 6
    share_parameters: false
    activation: "gelu"

  # head config
  # (required if model is of type encoder_with_head)
  head:
    # type of the head
    type: "sequence_classification"
    # optional arguments for head
    arguments:
      key: value

