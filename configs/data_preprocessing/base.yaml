## Config for data preprocessing

# HINT: You can use ${ENV_VAR:default_value} if you want to use environment variables in your config files

# data is a list of glob patterns pointing to files
# (usually cleaned .jsonl files)
data:
  - "example/training/*.jsonl"

# global seed (set to null if no seed is wanted)
seed: 22

# output directory of the preprocessed data
output_dir: "preprocessed/example"

# path to pretrained tokenizer to tokenize the sequences with or name of an existing tokenizer (e.g. char, byte, etc.)
tokenizer: "pretrained/tokenizer_bpe_wikidump.json"
# path to pretrained tokenizer to tokenize the target sequences with
# (if null then tokenizer from above is also used for target sequences)
target_tokenizer: null

# whether to pretokenize before applying the tokenizer, has the effect that substrings that are equal to
# some special tokens like <unk>, <sep> are properly tokenized and not seen as special tokens by the tokenizers
pretokenize: false

# optional list of preprocessing configurations
# if specified the data is preprocessed using the
# preprocessing configuration specified below
# (see whitespace_repair/utils/data.py for all available preprocessing functions)
preprocessing:
  - type: "whitespace"
    # optional arguments
    arguments:
      p: 0.1

# name of the lmdb database where the preprocessed data is stored
lmdb_name: "lmdb"

# stop after processing this number of (non-overflowing, see below) sequences, if null process all sequences
max_sequences: null
# set max number of sequence length a sequence can have
max_sequence_length: null
# whether to cut overflowing sequences to max_sequence_length or skip them
cut_overflowing: false