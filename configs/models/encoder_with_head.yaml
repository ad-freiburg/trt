model:
  name: "model_name"
  type: "encoder_with_head"

  pretrained: null
  start_from_checkpoint: null

  encoder:
    pretrained: null
    tokenizer: "char"

    embedding_dim: 768
    max_num_embeddings: 512
    learned_positional_embeddings: false
    norm_embeddings: true
    model_dim: 768
    feedforward_dim: 2048
    attention_heads: 8
    dropout: 0.1

    num_layers: 6
    share_parameters: true
    activation: "gelu"

  head:
    type: "sequence_classification"
    arguments:
      num_classes: 3